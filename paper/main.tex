\documentclass[runningheads]{llncs}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amssymb}
%\usepackage[square,numbers,sort&compress]{natbib}
\usepackage{xspace}
%% Useful packages
\usepackage{graphicx}
\usepackage{pdflscape}
\usepackage{multirow}
\usepackage{booktabs} % For formal tables
\usepackage{enumitem}
\usepackage{afterpage}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[square,numbers,sort]{natbib}
% Fixes for using natbib and llncs
\makeatletter
% required for natbib to have "References" printed and as section*, not chapter*
\renewcommand\bibsection%
{\section*{\refname\@mkboth{\MakeUppercase{\refname}}{\MakeUppercase{\refname}}}}
\renewcommand\@biblabel[1]{#1.}
\def\bibfont{\small}
\makeatother

% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\DeclareMathOperator*{\argmax}{arg\,max}

\hyphenation{%%% Merriam-Webster
  di-men-sion-al %%
  op-tical net-works semi-conduc-tor %%
  pher-o-mone non-dom-i-nance %%
  non-dom-i-nat-ed chro-mo-some %%
  sto-chas-tic make-span an-a-lys-ing}


\title{Mallows, Black-box combinatorial optimization, limited budget,  ???}
\author{Ekhiñe Irurozki\inst{1} \and Manuel López-Ibáñez\inst{2}\orcidID{0000-0001-9974-1295}}
\institute{
   Basque Center for Applied Mathematics\\
   \email{eirurozki@bcamath.org}
   \and
   University of Málaga, Málaga, Spain\\
   \email{manuel.lopez-ibanez@uma.es}
}
\date{}%

\begin{document}

\maketitle

\begin{abstract}
%The abstract should briefly summarize the contents of the paper in
%150--250 words.
  Black-box combinatorial optimization problems arise in practice when the
  objective function is evaluated by means of a simulator or a real-world
  experiment. In such cases, classical techniques such as mixed-integer
  programming and local search cannot be applied. Moreover, often each solution
  evaluation is expensive in terms of time or resources, thus only a limited
  number of evaluations is possible, typically several order of magnitude
  smaller than in white-box optimization problems. In the continuous case,
  Bayesian optimization, in particular using Gaussian processes, has proven
  very effective under these conditions. Much less research is available in the
  combinatorial case. In this paper, we propose and analyze an
  estimation-of-distribution (EDA) algorithm based on a Mallows probabilistic
  model and compare it with CEGO, a Bayesian optimization algorithm for
  combinatorial optimization. Experimental results show that the Bayesian
  algorithm is able to obtain very good solutions with very few evaluations,
  however, at a significant computational cost, whereas the proposed EDA
  outperforms CEGO when the number of solutions evaluated approaches 400, and
  it is significantly faster. These results suggest that the combination of
  Bayesian optimization and a Mallows-based EDA may be an interesting direction
  for future research.

\keywords{Combinatorial optimization \and Bayesian optimization \and Expensive black-box optimization \and Estimation of distribution algorithms}
\end{abstract}

\section{Introduction}

Motivation: Manuel

In many practical optimization problems, the objective function contexts
% minimize the uncertainty in the final solutions or

Related work: Ekhine

Our contribution: Ekhine
\begin{itemize}
\item 
\end{itemize}

\citet{ZaeStoBar2014:ppsn,ZaeStoFriFisNauBar2014}
\citet{PerLopStu2015si}

\citep{LopDubPerStuBir2016irace}

\section{Background}



Permutations are defined as bijections of the set $[n]$ integer onto itself. The set of all permutations of $n$ items is denoted as $S_n$ and has cardinality $n!$. We denote permutations with Greek letters with exception of the inverse permutation denoted as $e=1, 2, 3, \ldots,n$. We denote the composition of $\sigma$ and $\pi$ as $\sigma\pi$ and the inverse of $\sigma$ as $\sigma^{-1}$, for which the relation $\sigma\sigma_{-1}=e$ always holds. 

Distributions over permutations are functions that assign a probability value to each of the permutations in $S_n$, $p(\sigma)\in[0,1]$ \cite{critchlow91}. One of the most popular distributions is the Mallows Model (MM), which is considered as an analogous to the Gaussian distribution for permutations. The MM defines the probability of each permutation $\sigma$ as follows:

\begin{equation}
p(\sigma)=\frac{\exp(-\theta d(\sigma, \sigma_0))}{\psi}
\end{equation}

with two parameters, $\theta$ and $\sigma_0$, 
where permutation $\sigma_0$ a reference permutation that has the largest probability value, i.e., the mode of the distribution. The probability of every permutation $\sigma\in S_n$ decays exponentially as its distance $d(\sigma,\sigma_0)$ increases, and $\theta$, the dispersion parameter controls this decay. The distance $d(\sigma,\sigma_0)$ is the Kendall's-$\tau$ distance. The normalization constant $\psi$ can be easily computed for the Kendall's-$\tau$ distance as well as for the Hamming, Cayley and Ulam distance~\cite{Irurozki2016b}. 

One of the most common problems related ti probability distributions is that of learning the maximum likelihood parameters  given a sample of data $S$. For the MM, this problem tranlates to learning $\theta$ and $\sigma_0$ that best describe a sample of permutations. 

The learning process is divide in two stages: first, we estimate the central permutation of the distribution, $\hat\sigma0$ and, second, compute the dispersion parameter, $\hat\theta$. 

The exact maximum likelihood estimation is computationally hard~\cite{Dwork:2001:RAM:371920.372165}. However, the approximate learning requires polynomilal computational time and is guarantied to obtain high quality parameters~\cite{Caragiannis2013,Coppersmith:2010}. 

This process of sample $S$ is as follows: first, compute $\hat\sigma0$ with the Borda count algorithm. Borda orders the items $[n]$ by their \textit{Borda score} increasingly, where the Borda score $B(i)$ is the average of each position $i$, $B(i) \propto \sum_{t\in S}  \sigma_t(i)$. Second, the computation of $\theta$ is casted as a numerical optimization problem~\cite{Irurozki2016b}. 

Recently, it has been proposed an extension of Borda by the name of uBorda. uBorda considers a sample of permutations $S$ along with a weight $w(\sigma)$ for each permutation $\sigma$~\cite{}. Intuitively it is equivalent to replicating the permutations in the sample proportionally to their weight. It has been used to learn a MM in an evolving preference context. In this paper, we propose to use uBorda where each permutation in the sample $\sigma\in S$ is weighted by its fitness function $w(\sigma)=f(\sigma)$.




LOP: Ekhine

Quizás luego: PFSP y QAP

\section{Methods}


CEGO: Manuel.

Combinatorial Efficient Global Optimization
(CEGO)~\citep{ZaeStoFriFisNauBar2014} is an extension of the well-known EGO
method~\citep{JonSchWel98go} to unconstrained black-box combinatorial
optimization problems. In EGO, Gaussian process models are used as a surrogate
of the landscape of the expensive original problem. An optimization method
searches for solutions in the surrogate model by optimizing the expected
improvement criterion, which balances the expected mean and variance of the
chosen solution. Once a solution is chosen, it is evaluated on the actual
objective function and the result is used to update the surrogate-model,
hopefully increasing its predictive power.

CEGO replaces the Euclidean distance measure, used by the surrogate model in
EGO, with a distance measure appropriate to combinatorial
landscapes~\citep{ZaeStoBar2014:ppsn}, such as the Kendall distance for
permutations~\citep{?}. In CEGO, the surrogate model is explored by a GA with
crossover and mutation operators appropriate for the particular combinatorial
problem. The original paper notes that coupling the GA with local search does
not improve the results significantly since the model is anyway an inexact
estimation of the original objective
function~\citep[p.~875]{ZaeStoFriFisNauBar2014}.

What else do we need to say?

\newcommand{\minit}{\ensuremath{m_\text{ini}}\xspace}

\newcommand{\FEmax}{\ensuremath{FE_{\max}}}

Description of UMM: Ekhine

new approach: add sample, learn distri, sample distri

learn distri: uMM with param $\rho$ and binary search rho

sample: peakig distri

Quality guarantees


\section{Experimental setup}

We use the implementation of GECO provided by the
authors\footnote{\url{https://cran.r-project.org/package=CEGO}}. Following the
original paper~\citep{ZaeStoFriFisNauBar2014}, we use and the GA that optimizes
the surrogate models uses a population size of 20, crossover rate of 0.5,
mutation rate of 1, tournament selection of size 2 and probability of 0.9,
interchange mutation (i.e., exchanging two randomly selected elements of the
permutation) and cycle crossover for permutations. The budget of each run of
the GA is \textcolor{red}{$10^5$} evaluations using the
surrogate-model. Although it is never stated in the original paper, the
implementation of CEGO generates a set of initial solutions of size \minit by
means of a max-min-distance sequential design: new solutions are added to the
set sequentially by maximizing the minimum distance to solutions already in the
set. These initial solutions are then evaluated on the actual objective
function and the result is used to build the initial surrogate model.

Here we use $\minit=20$.

In all experiments, we consider a maximum budget of $\FEmax=400$ evaluations of
the actual objective function. In a white-box context, state-of-the-art
algorithms for the problems considered here typically evaluate
\textcolor{red}{hundreds? thousands?} of solutions, thus, the budget considered
here for the black-box context is extremely limited.

We implemented UMM in Python. The parameter settings that we use are...

Complete this with details of uMM: MANUEL

Computing system details: MANUEL

Describir como se generan las intancias de LOP: Ekhine

Describir las instancias: Ekhine


\section{Experimental analysis}

Bayesian optimization methods using a global GP model, such as CEGO, are known
to have trouble optimizing locally \citep{EriPeaGar2019scalable}. Our
intuition is that this problem becomes worse in rugged combinatorial
landscapes, where small steps may produce drastic changes.

\section{Conclusions}

\paragraph*{Acknowledgements.}

Thanks to Hao Wang (Leiden University) for pointing us to the arguments of
\citet{EriPeaGar2019scalable}.




\renewcommand{\doi}[1]{doi:\hspace{.16667em plus .08333em}\discretionary{}{}{}\href{https://doi.org/#1}{\urlstyle{rm}\nolinkurl{#1}}}
\bibliographystyle{splncs04nat}
\bibliography{optbib/abbrev,optbib/authors,optbib/journals,optbib/biblio,optbib/crossref}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
